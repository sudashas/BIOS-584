{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b5a1bfd72b35b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'Code', 'IndexBegin', 'IndexTag', 'LetterTable', 'Signal', 'Text', 'Type'])\n",
      "(3420, 400)\n",
      "(3420, 1)\n",
      "FRT keys: ['__header__', '__version__', '__globals__', 'BandLow', 'BandUpp', 'Code', 'IndexBegin', 'IndexTag', 'LetterTable', 'Signal', 'Text', 'Type']\n",
      "raw Signal shape: (1296, 400) raw Type shape: (1296, 1)\n",
      "processed Signal shape: (1296, 400) processed Type shape: (1296,)\n",
      "char_frt_size: 27\n",
      "trn_seq_size: 15 frt_seq_size: 4\n",
      "first 10 eeg_frt_type labels: [-1 -1  1  1 -1 -1 -1 -1 -1 -1]\n",
      "first 10 eeg_frt_code rows: [[ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 3]\n",
      " [11]\n",
      " [ 2]]\n",
      "Shapes: (3420, 1) (3420, 1) (3420, 1) (1296, 1) (1296, 1) (1296, 1)\n",
      "Created model-output arrays with shapes:\n",
      "logistic_y_trn (3420,) logistic_y_frt (1296,)\n",
      "lda_y_trn (3420,) lda_y_frt (1296,)\n",
      "svm_y_trn (3420,) svm_y_frt (1296,)\n",
      "\n",
      "Logistic (stimulus-level):\n",
      "  n_events: 3420\n",
      "  AUC: 0.9766752847029855\n",
      "  Accuracy @0.5: 0.9532163742690059\n",
      "  Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[2806   44]\n",
      " [ 116  454]]\n",
      "\n",
      "LDA (stimulus-level):\n",
      "  n_events: 3420\n",
      "  AUC: 0.9750766389658356\n",
      "  Accuracy @0.5: 0.9517543859649122\n",
      "  Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[2800   50]\n",
      " [ 115  455]]\n",
      "\n",
      "SVM (stimulus-level):\n",
      "  n_events: 3420\n",
      "  AUC: 0.9812376115727917\n",
      "  Accuracy @0.5: 0.9616959064327485\n",
      "  Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      "[[2826   24]\n",
      " [ 107  463]]\n",
      "Prepared FRT arrays: (1296, 1) (1296, 1) (1296, 1)\n",
      "char_frt_size: 27 frt_seq_size: 4\n",
      "Final shapes before streamline_predict: (3420, 1) (3420, 1) (3420, 1) (1296, 1) (1296, 1) (1296, 1)\n",
      "Logistic Regression on TRN:\n",
      "[['Z' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']\n",
      " ['C' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I']\n",
      " ['C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C']\n",
      " ['K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      " ['4' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]\n",
      "['T', 'H', 'E', '0', 'Q', 'U', 'I', 'C', 'K', '0', 'B', 'R', 'O', 'W', 'N', '0', 'F', 'O', 'X']\n",
      "Logistic Regression on FRT:\n",
      "[['H' 'H' 'H' 'T']\n",
      " ['8' '8' '8' '8']\n",
      " ['N' 'T' 'N' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['2' '8' '8' '8']\n",
      " ['0' '0' '0' '0']\n",
      " ['P' 'D' 'D' 'D']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['G' 'G' 'G' 'G']\n",
      " ['0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['0' '0' 'R' 'R']\n",
      " ['I' 'I' 'I' 'I']\n",
      " ['K' 'E' 'E' 'E']\n",
      " ['F' 'D' 'D' 'D']\n",
      " ['0' '0' '0' '0']\n",
      " ['N' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['T' 'B' 'B' 'B']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['E' 'E' 'E' 'E']]\n",
      "['T', '8', 'T', 'H', 'E', '0', '8', '0', 'D', 'O', 'G', '0', 'B', 'U', 'R', 'I', 'E', 'D', '0', 'T', 'H', 'E', '0', 'B', 'O', 'N', 'E']\n",
      "LDA on TRN:\n",
      "[['Z' 'Z' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']\n",
      " ['C' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I']\n",
      " ['C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C']\n",
      " ['K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['H' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      " ['4' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]\n",
      "['T', 'H', 'E', '0', 'Q', 'U', 'I', 'C', 'K', '0', 'B', 'R', 'O', 'W', 'N', '0', 'F', 'O', 'X']\n",
      "LDA on FRT:\n",
      "[['H' 'H' 'H' 'H']\n",
      " ['8' '8' '8' '8']\n",
      " ['N' 'N' 'N' 'T']\n",
      " ['H' 'B' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['2' '8' '8' '8']\n",
      " ['0' '0' '0' '0']\n",
      " ['D' 'D' 'D' 'D']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['G' 'G' 'G' 'G']\n",
      " ['4' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['4' '4' 'R' 'R']\n",
      " ['I' 'I' 'I' 'I']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['F' 'D' 'D' 'D']\n",
      " ['0' '0' '0' '0']\n",
      " ['N' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['T' 'B' 'B' 'B']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['E' 'E' 'E' 'E']]\n",
      "['T', '8', 'T', 'H', 'E', '0', '8', '0', 'D', 'O', 'G', '0', 'B', 'U', 'R', 'I', 'E', 'D', '0', 'T', 'H', 'E', '0', 'B', 'O', 'N', 'E']\n",
      "Support Vector Machine on TRN:\n",
      "[['Z' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']\n",
      " ['I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I']\n",
      " ['C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C']\n",
      " ['K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]\n",
      "['T', 'H', 'E', '0', 'Q', 'U', 'I', 'C', 'K', '0', 'B', 'R', 'O', 'W', 'N', '0', 'F', 'O', 'X']\n",
      "Support Vector Machine on FRT:\n",
      "[['H' 'H' 'H' 'T']\n",
      " ['D' '8' '8' '8']\n",
      " ['T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' 'R' '0']\n",
      " ['2' '8' '8' '8']\n",
      " ['0' '0' '0' '0']\n",
      " ['D' 'D' 'D' 'D']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['G' 'G' 'G' 'G']\n",
      " ['0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['R' 'R' 'R' 'R']\n",
      " ['7' '7' '7' 'I']\n",
      " ['K' 'E' 'E' 'E']\n",
      " ['R' 'D' 'D' 'D']\n",
      " ['0' '0' '0' '0']\n",
      " ['N' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'K' 'E' 'E']\n",
      " ['9' '0' '0' '0']\n",
      " ['T' 'B' 'B' 'B']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['E' 'E' 'E' 'E']]\n",
      "['T', '8', 'T', 'H', 'E', '0', '8', '0', 'D', 'O', 'G', '0', 'B', 'U', 'R', 'I', 'E', 'D', '0', 'T', 'H', 'E', '0', 'B', 'O', 'N', 'E']\n",
      "Logistic trn accuracy: [0.84210526 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "lda trn accuracy: [0.78947368 0.94736842 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "svm trn accuracy: [0.94736842 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "log frt acc: [0.66666667 0.92592593 0.92592593 1.        ]\n",
      "lds frt acc: [0.7037037  0.85185185 0.92592593 0.96296296]\n",
      "svm frt acc: [0.66666667 0.88888889 0.88888889 1.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Dr. Ma, I converted this to a jupyter notebook within pycharm and am working on this in jupyter notebook as I prefer the UI much more than pycharm. Is this okay? \n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from self_py_fun.HW10Fun import *\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "# In HW7, you have the chance to visualize a truncated EEG dataset stratified by\n",
    "# target and non-target stimulus type.\n",
    "#\n",
    "# The fundamental problem of P300 ERP-BCI speller system is to perform a binary classification.\n",
    "#\n",
    "# In HW10, you are asked to implement the binary classification using various methods,\n",
    "# and evaluate the model performance with a testing dataset.\n",
    "#\n",
    "# You will use K114_001_BCI_TRN_Truncated_Data_0.5_6.mat as a training set, and\n",
    "# K114_001_BCI_FRT_Truncated_Data_0.5_6.mat as a testing set.\n",
    "#\n",
    "# Notice that here, we do not split training/testing within K114_001_BCI_TRN_Truncated_Data_0.5_6.mat\n",
    "# because each row is not entirely independent of each other due to the special structure of the dataset.\n",
    "\n",
    "# Global constants:\n",
    "np.random.seed(100)\n",
    "bp_low = 0.5\n",
    "bp_upp = 6\n",
    "electrode_num = 16\n",
    "# Change the following directory to your own one.\n",
    "parent_dir = os.getcwd()\n",
    "parent_data_dir = '{}/data'.format(parent_dir)\n",
    "time_index = np.linspace(0, 800, 25)\n",
    "electrode_name_ls = ['F3', 'Fz', 'F4', 'T7', 'C3', 'Cz', 'C4', 'T8', 'CP3', 'CP4', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "subject_name = 'K114'\n",
    "# create a new folder called K114\n",
    "subject_dir = '{}/{}'.format(parent_dir, subject_name)\n",
    "if not os.path.exists(subject_dir):\n",
    "    os.mkdir(subject_dir)\n",
    "\n",
    "char_trn = 'THE0QUICK0BROWN0FOX' #19\n",
    "char_trn_size = len(char_trn)\n",
    "print(char_trn_size) #also 19\n",
    "# Step 1: Import dataset\n",
    "# Step 1.1: TRN dataset\n",
    "trn_data_name = '{}_001_BCI_TRN_Truncated_Data_{}_{}'.format(subject_name, bp_low, bp_upp)\n",
    "trn_data_dir = '{}/{}.mat'.format(parent_data_dir, trn_data_name)\n",
    "eeg_trn_obj = sio.loadmat(trn_data_dir)\n",
    "\n",
    "# eeg_trn_obj is a dictionary!\n",
    "print(eeg_trn_obj.keys())\n",
    "eeg_trn_signal = eeg_trn_obj['Signal']\n",
    "print(eeg_trn_signal.shape) # 3420, 400\n",
    "eeg_trn_type = eeg_trn_obj['Type']\n",
    "print(eeg_trn_type.shape) # 3420, 1\n",
    "eeg_trn_type = np.squeeze(eeg_trn_type, axis=1)\n",
    "\n",
    "# Step 1.2: FRT dataset\n",
    "# The following code should be completed by students themselves.\n",
    "# you should be able to obtain relevant data files named\n",
    "# eeg_frt_signal and eeg_frt_type\n",
    "# Write your own code below:\n",
    "\n",
    "# Cell: load FRT dataset and prepare related variables\n",
    "from pathlib import Path\n",
    "\n",
    "parent_data_dir = Path(parent_data_dir)\n",
    "\n",
    "subject_name = globals().get(\"subject_name\", \"K114\")\n",
    "bp_low = globals().get(\"bp_low\", 0.5)\n",
    "bp_upp = globals().get(\"bp_upp\", 6)\n",
    "\n",
    "frt_data_name = f\"{subject_name}_001_BCI_FRT_Truncated_Data_{bp_low}_{bp_upp}\"\n",
    "frt_path = parent_data_dir / f\"{frt_data_name}.mat\"\n",
    "\n",
    "# Existence check\n",
    "if not frt_path.exists():\n",
    "    raise FileNotFoundError(f\"FRT .mat not found at: {frt_path}\")\n",
    "\n",
    "# Load .mat\n",
    "eeg_frt_obj = sio.loadmat(str(frt_path))\n",
    "print(\"FRT keys:\", list(eeg_frt_obj.keys()))\n",
    "\n",
    "# Extract Signal and Type with defensive checks\n",
    "if 'Signal' not in eeg_frt_obj or 'Type' not in eeg_frt_obj:\n",
    "    raise KeyError(\"Expected keys 'Signal' and 'Type' not found in the FRT .mat file.\")\n",
    "\n",
    "eeg_frt_signal = eeg_frt_obj['Signal']\n",
    "eeg_frt_type = eeg_frt_obj['Type']\n",
    "\n",
    "# Ensure shapes and dtypes are convenient\n",
    "print(\"raw Signal shape:\", eeg_frt_signal.shape, \"raw Type shape:\", eeg_frt_type.shape)\n",
    "\n",
    "# If Type has shape (n,1) or (n, ), squeeze to (n,)\n",
    "eeg_frt_type = np.squeeze(eeg_frt_type)\n",
    "# Convert to integer labels if appropriate\n",
    "try:\n",
    "    eeg_frt_type = eeg_frt_type.astype(int)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# If Signal is 3D (n, channels, times), flatten to (n, channels*times) for classic classifiers\n",
    "if eeg_frt_signal.ndim == 3:\n",
    "    n_frt = eeg_frt_signal.shape[0]\n",
    "    eeg_frt_signal = eeg_frt_signal.reshape(n_frt, -1)\n",
    "\n",
    "print(\"processed Signal shape:\", eeg_frt_signal.shape, \"processed Type shape:\", eeg_frt_type.shape)\n",
    "\n",
    "# Code and Text fields (if present)\n",
    "eeg_frt_code = eeg_frt_obj.get('Code', None)\n",
    "eeg_frt_text = eeg_frt_obj.get('Text', None)\n",
    "\n",
    "# Derive char_frt using helper if available, otherwise fallback to simple conversion\n",
    "char_frt = None\n",
    "if 'convert_raw_char_to_alphanumeric_stype' in globals():\n",
    "    try:\n",
    "        char_frt = convert_raw_char_to_alphanumeric_stype(eeg_frt_text)\n",
    "    except Exception:\n",
    "        char_frt = None\n",
    "\n",
    "if char_frt is None:\n",
    "    # Fallback: if Text is an array of characters or a string, try to convert\n",
    "    if eeg_frt_text is None:\n",
    "        raise ValueError(\"No 'Text' field found in FRT .mat and helper conversion failed.\")\n",
    "    # handle common MATLAB string formats\n",
    "    if isinstance(eeg_frt_text, np.ndarray):\n",
    "        # try to join characters or decode bytes\n",
    "        try:\n",
    "            # if it's an array of shape (1,) with a string\n",
    "            if eeg_frt_text.size == 1:\n",
    "                raw = eeg_frt_text.flatten()[0]\n",
    "                char_frt = list(str(raw))\n",
    "            else:\n",
    "                # flatten and join rows\n",
    "                char_frt = [str(x).strip() for x in eeg_frt_text.flatten()]\n",
    "        except Exception:\n",
    "            char_frt = list(str(eeg_frt_text))\n",
    "    else:\n",
    "        char_frt = list(str(eeg_frt_text))\n",
    "\n",
    "# Finalize char_frt and sizes\n",
    "char_frt = list(char_frt)\n",
    "char_frt_size = len(char_frt)\n",
    "print(\"char_frt_size:\", char_frt_size)\n",
    "\n",
    "# Compute sequence sizes using the same logic as training code\n",
    "# (adjust divisor if your dataset uses a different grouping than 12)\n",
    "trn_seq_size = globals().get(\"trn_seq_size\", None)\n",
    "if trn_seq_size is None and 'eeg_trn_signal' in globals() and 'char_trn_size' in globals():\n",
    "    trn_seq_size = int(eeg_trn_signal.shape[0] / char_trn_size / 12)\n",
    "\n",
    "frt_seq_size = int(eeg_frt_signal.shape[0] / char_frt_size / 12)\n",
    "print(\"trn_seq_size:\", trn_seq_size, \"frt_seq_size:\", frt_seq_size)\n",
    "\n",
    "# Print a small sanity check of the first few labels and codes\n",
    "print(\"first 10 eeg_frt_type labels:\", eeg_frt_type[:10])\n",
    "if eeg_frt_code is not None:\n",
    "    print(\"first 10 eeg_frt_code rows:\", eeg_frt_code[:10])\n",
    "\n",
    "\n",
    "\n",
    "# You have completed the exploratory data analysis in HW7 and HW8.\n",
    "# The dataset has been carefully reviewed by Dr. Jane E. Huggins,\n",
    "# so we do not need to worry about missing, outliers, errors of the dataset.\n",
    "\n",
    "# Step 2: Fit classification models\n",
    "# You will try the following methods:\n",
    "# Logistic Regression,\n",
    "# Linear Discriminant Analysis,\n",
    "# Support Vector Machine (sometimes called support vector classification)\n",
    "# You do not need to modify the parameters of each classifier\n",
    "# except for LogisticRegression: set max_iter=1000\n",
    "# Write your own code below:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ensure the required data variables exist\n",
    "X_trn = eeg_trn_signal\n",
    "y_trn = np.squeeze(eeg_trn_type).astype(int)\n",
    "\n",
    "X_frt = eeg_frt_signal\n",
    "y_frt = np.squeeze(eeg_frt_type).astype(int)\n",
    "\n",
    "# Flatten if needed (AI says this is where my error was before)\n",
    "if X_trn.ndim == 3:\n",
    "    X_trn = X_trn.reshape(X_trn.shape[0], -1)\n",
    "if X_frt.ndim == 3:\n",
    "    X_frt = X_frt.reshape(X_frt.shape[0], -1)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_trn_scaled = scaler.fit_transform(X_trn)\n",
    "X_frt_scaled = scaler.transform(X_frt)\n",
    "\n",
    "# Ensure model-output arrays are column vectors (n_events, 1) for streamline_predict\n",
    "import numpy as np\n",
    "\n",
    "logistic_y_trn = np.asarray(logistic_y_trn).ravel()[:, np.newaxis]\n",
    "lda_y_trn      = np.asarray(lda_y_trn).ravel()[:, np.newaxis]\n",
    "svm_y_trn      = np.asarray(svm_y_trn).ravel()[:, np.newaxis]\n",
    "\n",
    "logistic_y_frt = np.asarray(logistic_y_frt).ravel()[:, np.newaxis]\n",
    "lda_y_frt      = np.asarray(lda_y_frt).ravel()[:, np.newaxis]\n",
    "svm_y_frt      = np.asarray(svm_y_frt).ravel()[:, np.newaxis]\n",
    "\n",
    "print(\"Shapes:\", logistic_y_trn.shape, lda_y_trn.shape, svm_y_trn.shape,\n",
    "      logistic_y_frt.shape, lda_y_frt.shape, svm_y_frt.shape)\n",
    "\n",
    "# Logistic Regression\n",
    "log_clf = LogisticRegression(max_iter=1000, random_state=100)\n",
    "log_clf.fit(X_trn_scaled, y_trn)\n",
    "logistic_y_trn = log_clf.predict_proba(X_trn_scaled)[:, 1]\n",
    "logistic_y_frt = log_clf.predict_proba(X_frt_scaled)[:, 1]\n",
    "\n",
    "# LDA\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "lda_clf.fit(X_trn_scaled, y_trn)\n",
    "if hasattr(lda_clf, \"predict_proba\"):\n",
    "    lda_y_trn = lda_clf.predict_proba(X_trn_scaled)[:, 1]\n",
    "    lda_y_frt = lda_clf.predict_proba(X_frt_scaled)[:, 1]\n",
    "else:\n",
    "    lda_y_trn = lda_clf.decision_function(X_trn_scaled)\n",
    "    lda_y_frt = lda_clf.decision_function(X_frt_scaled)\n",
    "\n",
    "svm_clf = SVC(kernel='rbf', probability=True, random_state=100)\n",
    "svm_clf.fit(X_trn_scaled, y_trn)\n",
    "svm_y_trn = svm_clf.predict_proba(X_trn_scaled)[:, 1]\n",
    "svm_y_frt = svm_clf.predict_proba(X_frt_scaled)[:, 1]\n",
    "\n",
    "trained_models = {\n",
    "    \"logistic\": log_clf,\n",
    "    \"lda\": lda_clf,\n",
    "    \"svm\": svm_clf,\n",
    "    \"scaler\": scaler\n",
    "}\n",
    "\n",
    "#Printing out step 2 results because this is all in 1 code chunk and the output is LONG\n",
    "print(\"Created model-output arrays with shapes:\")\n",
    "print(\"logistic_y_trn\", logistic_y_trn.shape, \"logistic_y_frt\", logistic_y_frt.shape)\n",
    "print(\"lda_y_trn\", np.shape(lda_y_trn), \"lda_y_frt\", np.shape(lda_y_frt))\n",
    "print(\"svm_y_trn\", svm_y_trn.shape, \"svm_y_frt\", svm_y_frt.shape)\n",
    "\n",
    "\n",
    "# Step 3: Evaluate model performance on both TRN and FRT files\n",
    "# Step 3.1: Prediction accuracy on TRN files\n",
    "# We will compute the probability of each stimulus with .predict_proba()\n",
    "# before we convert the stimulus-level probability to character-level probability.\n",
    "# You are asked to generate stimulus-level probability for each method on TRN files,\n",
    "# denoted as logistic_y_trn, lda_y_trn, and svm_y_trn.\n",
    "# Write your own code below:\n",
    "# Step 3.1 — stimulus-level evaluation on TRN\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Ensure data variables exist\n",
    "assert 'eeg_trn_signal' in globals() and 'eeg_trn_type' in globals(), \"Load TRN data first.\"\n",
    "\n",
    "# Prepare binary labels for metrics: target==1 -> True, non-target otherwise\n",
    "y_trn_raw = np.squeeze(eeg_trn_type)\n",
    "y_trn_bin = (y_trn_raw == 1).astype(int)\n",
    "\n",
    "# If any model-output arrays are missing, try to compute them using trained models in trained_models\n",
    "if 'logistic_y_trn' not in globals() or 'lda_y_trn' not in globals() or 'svm_y_trn' not in globals():\n",
    "    print(\"Some model-output arrays missing; attempting to compute from trained_models if available.\")\n",
    "    # require trained_models and scaler\n",
    "    if 'trained_models' in globals() and 'scaler' in globals():\n",
    "        scaler = trained_models.get(\"scaler\", globals().get(\"scaler\"))\n",
    "        X_trn = eeg_trn_signal\n",
    "        if X_trn.ndim == 3:\n",
    "            X_trn = X_trn.reshape(X_trn.shape[0], -1)\n",
    "        X_trn_scaled = scaler.transform(X_trn) if hasattr(scaler, \"transform\") else X_trn\n",
    "        # logistic\n",
    "        if 'logistic_y_trn' not in globals() and 'logistic' in trained_models:\n",
    "            clf = trained_models['logistic']\n",
    "            logistic_y_trn = clf.predict_proba(X_trn_scaled)[:, 1]\n",
    "        # lda\n",
    "        if 'lda_y_trn' not in globals() and 'lda' in trained_models:\n",
    "            clf = trained_models['lda']\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                lda_y_trn = clf.predict_proba(X_trn_scaled)[:, 1]\n",
    "            else:\n",
    "                lda_y_trn = clf.decision_function(X_trn_scaled)\n",
    "        # svm\n",
    "        if 'svm_y_trn' not in globals() and 'svm' in trained_models:\n",
    "            clf = trained_models['svm']\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                svm_y_trn = clf.predict_proba(X_trn_scaled)[:, 1]\n",
    "            else:\n",
    "                svm_y_trn = clf.decision_function(X_trn_scaled)\n",
    "    else:\n",
    "        raise RuntimeError(\"Model outputs missing and no trained_models/scaler available to compute them.\")\n",
    "\n",
    "# Collect methods and arrays\n",
    "methods = {\n",
    "    \"Logistic\": globals().get(\"logistic_y_trn\"),\n",
    "    \"LDA\"     : globals().get(\"lda_y_trn\"),\n",
    "    \"SVM\"     : globals().get(\"svm_y_trn\")\n",
    "}\n",
    "\n",
    "# Evaluate each method at stimulus level\n",
    "for name, scores in methods.items():\n",
    "    if scores is None:\n",
    "        print(f\"{name}: no scores found, skipping.\")\n",
    "        continue\n",
    "    # Ensure 1D numpy array\n",
    "    scores = np.asarray(scores).ravel()\n",
    "    if scores.shape[0] != y_trn_bin.shape[0]:\n",
    "        print(f\"{name}: length mismatch (scores {scores.shape[0]} vs labels {y_trn_bin.shape[0]}). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # AUC (requires both classes present)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_trn_bin, scores)\n",
    "    except Exception as e:\n",
    "        auc = f\"NA ({e})\"\n",
    "\n",
    "    # Accuracy at threshold 0.5 (convert scores to predicted labels)\n",
    "    y_pred = (scores >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_trn_bin, y_pred)\n",
    "    cm = confusion_matrix(y_trn_bin, y_pred)\n",
    "\n",
    "    print(f\"\\n{name} (stimulus-level):\")\n",
    "    print(\"  n_events:\", scores.shape[0])\n",
    "    print(\"  AUC:\", auc)\n",
    "    print(\"  Accuracy @0.5:\", acc)\n",
    "    print(\"  Confusion matrix (rows=true [0,1], cols=pred [0,1]):\")\n",
    "    print(cm)\n",
    "\n",
    "# Step 3.2: Prediction accuracy on FRT files\n",
    "# Similarly, you are asked to generate stimulus-level probability for each method on FRT files,\n",
    "# denoted as logistic_y_frt, lda_y_frt, and svm_y_frt.\n",
    "# Write your own code below:\n",
    "\n",
    "\n",
    "# Step 3.2: ensure stimulus-level probabilities and FRT metadata exist\n",
    "import numpy as np\n",
    "\n",
    "# --- basic checks ---\n",
    "assert 'eeg_frt_signal' in globals() and 'eeg_frt_obj' in globals(), \"Load FRT .mat first.\"\n",
    "\n",
    "# expose code field and text/char info used by Step 4\n",
    "eeg_frt_code = eeg_frt_obj['Code']\n",
    "# prefer helper conversion if available\n",
    "if 'convert_raw_char_to_alphanumeric_stype' in globals():\n",
    "    try:\n",
    "        char_frt = convert_raw_char_to_alphanumeric_stype(eeg_frt_obj['Text'])\n",
    "    except Exception:\n",
    "        char_frt = list(str(eeg_frt_obj.get('Text', '')))\n",
    "else:\n",
    "    # fallback: try to extract readable text from the Text field\n",
    "    raw_text = eeg_frt_obj.get('Text', '')\n",
    "    if isinstance(raw_text, np.ndarray) and raw_text.size == 1:\n",
    "        char_frt = list(str(raw_text.flatten()[0]))\n",
    "    elif isinstance(raw_text, np.ndarray):\n",
    "        char_frt = [str(x).strip() for x in raw_text.flatten()]\n",
    "    else:\n",
    "        char_frt = list(str(raw_text))\n",
    "\n",
    "char_frt = list(char_frt)\n",
    "char_frt_size = len(char_frt)\n",
    "frt_seq_size = int(eeg_frt_signal.shape[0] / char_frt_size / 12)\n",
    "\n",
    "# --- compute/confirm model-output arrays for FRT ---\n",
    "# If already present, keep them; otherwise compute from trained models or individual classifiers\n",
    "def _ensure_scores(name, clf_key=None, clf_var=None):\n",
    "    if name in globals():\n",
    "        arr = np.asarray(globals()[name]).ravel()\n",
    "        return arr\n",
    "    # try trained_models dict first\n",
    "    if 'trained_models' in globals() and clf_key in trained_models:\n",
    "        clf = trained_models[clf_key]\n",
    "    elif clf_var is not None and clf_var in globals():\n",
    "        clf = globals()[clf_var]\n",
    "    else:\n",
    "        return None\n",
    "    X = eeg_frt_signal\n",
    "    if X.ndim == 3:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    scaler = trained_models.get('scaler') if 'trained_models' in globals() else globals().get('scaler', None)\n",
    "    if scaler is not None and hasattr(scaler, \"transform\"):\n",
    "        Xs = scaler.transform(X)\n",
    "    else:\n",
    "        Xs = X\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        return clf.predict_proba(Xs)[:, 1].ravel()\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        return clf.decision_function(Xs).ravel()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "logistic_y_frt = _ensure_scores('logistic_y_frt', clf_key='logistic', clf_var='log_clf')\n",
    "lda_y_frt      = _ensure_scores('lda_y_frt',      clf_key='lda',      clf_var='lda_clf')\n",
    "svm_y_frt      = _ensure_scores('svm_y_frt',      clf_key='svm',      clf_var='svm_clf')\n",
    "\n",
    "# final sanity: raise clear error if any required score is still missing\n",
    "missing_scores = [n for n in ['logistic_y_frt','lda_y_frt','svm_y_frt'] if globals().get(n) is None]\n",
    "if missing_scores:\n",
    "    raise RuntimeError(f\"Missing FRT model-output arrays: {missing_scores}. Re-run Step 2 to train models or ensure trained_models/scaler exist.\")\n",
    "\n",
    "# coerce to 1D numpy arrays\n",
    "# after computing logistic_y_frt, lda_y_frt, svm_y_frt\n",
    "import numpy as np\n",
    "logistic_y_frt = np.asarray(logistic_y_frt).ravel()[:, np.newaxis]\n",
    "lda_y_frt      = np.asarray(lda_y_frt).ravel()[:, np.newaxis]\n",
    "svm_y_frt      = np.asarray(svm_y_frt).ravel()[:, np.newaxis]\n",
    "\n",
    "\n",
    "# quick confirmation print (safe, minimal)\n",
    "print(\"Prepared FRT arrays:\", logistic_y_frt.shape, lda_y_frt.shape, svm_y_frt.shape)\n",
    "print(\"char_frt_size:\", char_frt_size, \"frt_seq_size:\", frt_seq_size)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Convert binary classification probability to character-level accuracy\n",
    "# This involves advanced data manipulation, so you do not need to write any new code.\n",
    "# Please run the following code to view the final results.\n",
    "eeg_trn_code = eeg_trn_obj['Code']\n",
    "eeg_frt_code = eeg_frt_obj['Code']\n",
    "char_frt = convert_raw_char_to_alphanumeric_stype(eeg_frt_obj['Text'])\n",
    "# raw format is different from the current 6x6 layout characters.\n",
    "char_frt_size = len(char_frt)\n",
    "frt_seq_size = int(eeg_frt_signal.shape[0]/char_frt_size/12)\n",
    "\n",
    "# --- Ensure model-output arrays are column vectors right before evaluation ---\n",
    "import numpy as np\n",
    "\n",
    "for name in ['logistic_y_trn','lda_y_trn','svm_y_trn','logistic_y_frt','lda_y_frt','svm_y_frt']:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"{name} not found in globals() — run Step 2/3 first.\")\n",
    "    arr = np.asarray(globals()[name]).ravel()\n",
    "    globals()[name] = arr[:, np.newaxis]\n",
    "\n",
    "# Confirm shapes\n",
    "print(\"Final shapes before streamline_predict:\",\n",
    "      logistic_y_trn.shape, lda_y_trn.shape, svm_y_trn.shape,\n",
    "      logistic_y_frt.shape, lda_y_frt.shape, svm_y_frt.shape)\n",
    "\n",
    "# Logistic regression\n",
    "print('Logistic Regression on TRN:')\n",
    "logistic_letter_mat_trn, logistic_letter_prob_mat_trn = streamline_predict(\n",
    "    logistic_y_trn, eeg_trn_type, eeg_trn_code, char_trn_size, trn_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(logistic_letter_mat_trn)\n",
    "print(list(char_trn)) # This is the true spelling characters for training set!\n",
    "logistic_trn_accuracy = np.mean(logistic_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "\n",
    "print('Logistic Regression on FRT:')\n",
    "logistic_letter_mat_frt, logistic_letter_prob_mat_frt = streamline_predict(\n",
    "    logistic_y_frt, eeg_frt_type, eeg_frt_code, char_frt_size, frt_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(logistic_letter_mat_frt)\n",
    "print(list(char_frt)) # This is the true spelling characters for testing set!\n",
    "logistic_frt_accuracy = np.mean(logistic_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "# LDA:\n",
    "print('LDA on TRN:')\n",
    "lda_letter_mat_trn, lda_letter_prob_mat_trn = streamline_predict(\n",
    "    lda_y_trn, eeg_trn_type, eeg_trn_code, char_trn_size, trn_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(lda_letter_mat_trn)\n",
    "print(list(char_trn)) # This is the true spelling characters for training set!\n",
    "lda_trn_accuracy = np.mean(lda_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "\n",
    "print('LDA on FRT:')\n",
    "lda_letter_mat_frt, lda_letter_prob_mat_frt = streamline_predict(\n",
    "    lda_y_frt, eeg_frt_type, eeg_frt_code, char_frt_size, frt_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(lda_letter_mat_frt)\n",
    "print(list(char_frt)) # This is the true spelling characters for testing set!\n",
    "lda_frt_accuracy = np.mean(lda_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "# SVM:\n",
    "print('Support Vector Machine on TRN:')\n",
    "svm_letter_mat_trn, svm_letter_prob_mat_trn = streamline_predict(\n",
    "    svm_y_trn, eeg_trn_type, eeg_trn_code, char_trn_size, trn_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(svm_letter_mat_trn)\n",
    "print(list(char_trn)) # This is the true spelling characters for training set!\n",
    "svm_trn_accuracy = np.mean(svm_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "\n",
    "print('Support Vector Machine on FRT:')\n",
    "svm_letter_mat_frt, svm_letter_prob_mat_frt = streamline_predict(\n",
    "    svm_y_frt, eeg_frt_type, eeg_frt_code, char_frt_size, frt_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(svm_letter_mat_frt)\n",
    "print(list(char_frt)) # This is the true spelling characters for training set!\n",
    "svm_frt_accuracy = np.mean(svm_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "\n",
    "print(\"Logistic trn accuracy:\",logistic_trn_accuracy) # I added labels so I know what I'm looking at if that's okay\n",
    "print(\"lda trn accuracy:\",lda_trn_accuracy)\n",
    "print(\"svm trn accuracy:\",svm_trn_accuracy)\n",
    "\n",
    "print(\"log frt acc:\",logistic_frt_accuracy)\n",
    "print(\"lds frt acc:\",lda_frt_accuracy)\n",
    "print(\"svm frt acc:\",svm_frt_accuracy)\n",
    "\n",
    "\n",
    "# Remember to answer two questions below:\n",
    "\n",
    "# What do rows 122, 131, 141, 150, 160, and 169 do? Briefly answer the question below:\n",
    "# In case that your row IDs are messed up when you start to fill in the blank,\n",
    "# I attach the lines of code for your reference.\n",
    "# logistic_trn_accuracy = np.mean(logistic_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0) \n",
    "    # this line gives us the results of the logistic regression run on the per character accuracy (spellcheck, essentially) for the training dataset\n",
    "# logistic_frt_accuracy = np.mean(logistic_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "    #  this line gives us the results of the logistic regression run on the per character accuracy (spellcheck, essentially) for the test dataset\n",
    "# lda_trn_accuracy = np.mean(lda_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "    #calculates the per character accuracy vector using LDA method on a Gaussian distribution for the training dataset\n",
    "# lda_frt_accuracy = np.mean(lda_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "    #calculates the per character accuracy vector using LDA method on a Gaussian distribution for the test dataset\n",
    "# svm_trn_accuracy = np.mean(svm_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "    #the per character accuracy vector (fraction of characters correctly decoded per attempt) for the training dataset\n",
    "# svm_frt_accuracy = np.mean(svm_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "    #the per character accuracy vector (fraction of characters correctly decoded per attempt) for the test dataset\n",
    "\n",
    "# Step 5: Summary\n",
    "# Which method performs the best? Why? \n",
    "#for this dataset, SVM performs the best. It's accuracy metrics are the highest with a AUC of 0.981 and an accuracy of 0.962.\n",
    "# SVM also has the fewest false positives and false negatives compared with logistic and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910c1bc-c1c1-440e-8226-5198447553b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
