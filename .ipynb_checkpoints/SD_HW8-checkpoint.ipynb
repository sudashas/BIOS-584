{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d99a94a-c380-4709-8873-caf66152aa76",
   "metadata": {},
   "source": [
    "# HW8 (20')\n",
    "\n",
    "<font size='4'>\n",
    "\n",
    "For this assignment, it is a combination of jupyter notebook assignment and python scripts.\n",
    "\n",
    "For Q1, please upload your outputs including codes and graphics to your own GitHub repository. <br> You will need to disclose your GitHub repository below.\n",
    "\n",
    "For Q2, please submit this jupyter notebook as an HTML or PDF file.\n",
    "\n",
    "First of all, print your name (First and Last) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a37202-5fb1-44da-8ce4-756934fbd067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suhas Das\n"
     ]
    }
   ],
   "source": [
    "print('Suhas Das')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bd5ad-c5ef-4515-8dea-d3f87f17f4b0",
   "metadata": {},
   "source": [
    "## 0. Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f72739f-c63c-4a6e-a5d0-c7ddc0f05242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in ./.venv/lib/python3.13/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in ./.venv/lib/python3.13/site-packages (from statsmodels) (2.3.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in ./.venv/lib/python3.13/site-packages (from statsmodels) (1.16.3)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in ./.venv/lib/python3.13/site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in ./.venv/lib/python3.13/site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in ./.venv/lib/python3.13/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff8980-aabc-4e7b-a846-fcc80c3920cf",
   "metadata": {},
   "source": [
    "## Q1. Convert your HW7 to python scripts. (10')\n",
    "\n",
    "<font size='4'>\n",
    "\n",
    "- Under your working directory, there should be a folder called `self_py_fun`.\n",
    "- Create a new python file called `HW8Fun.py` and move previously defined functions `produce_trun_mean_cov()`, `plot_trunc_mean()`, and `plot_trunc_cov()` to that file. Make sure you import proper packages.\n",
    "- Create another main file `HW8_main.py`.\n",
    "- Import relevant packages, modules, and/or function.\n",
    "- Copy the global variables and call your functions inside `HW8_main.py`.\n",
    "- A major difference compared to HW7 is that you are asked to save those figures to your local working environment.\n",
    "    - Create a new directory `K114` under your current working directory.\n",
    "    - For mean functions, please save it as a `Mean.png` output using `plt.savefig()` function.\n",
    "    - The changes should be made within `HW8Fun.py` rather than `HW8_main.py`.\n",
    "    - For covariance matrices, please save them as `Covariance_Target.png`, `Covariance_Non-Target.png`, and `Covariance_All.png` outputs using the same function above.\n",
    "    - To summarize, there should be **four** figures under `K114` folder.\n",
    "- Upload your entire work to your GitHub repository via push button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cb9623e-17b9-4062-bdd0-1f19a9d3f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your GitHub repository link below in the Markdown chunk. Remember to make it public and make the link clickable.\n",
    "# Do not include sensitive information in your GitHub repository.\n",
    "#HW8fun.py\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_dir = '/Users/suhasdas/Documents/GitHub/BIOS-584/K114_2'\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "fun_code = '''\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def produce_trun_mean_cov(input_signal, input_type, E_val):\n",
    "    length = input_signal.shape[1] // E_val\n",
    "    tar = input_signal[input_type == 1]\n",
    "    ntar = input_signal[input_type == -1]\n",
    "    tar_mean = np.zeros((E_val, length))\n",
    "    ntar_mean = np.zeros((E_val, length))\n",
    "    tar_cov = np.zeros((E_val, length, length))\n",
    "    ntar_cov = np.zeros((E_val, length, length))\n",
    "    all_cov = np.zeros((E_val, length, length))\n",
    "    for e in range(E_val):\n",
    "        s = e * length\n",
    "        e_ = (e + 1) * length\n",
    "        tar_mean[e] = np.mean(tar[:, s:e_], axis=0)\n",
    "        ntar_mean[e] = np.mean(ntar[:, s:e_], axis=0)\n",
    "        tar_cov[e] = np.cov(tar[:, s:e_], rowvar=False)\n",
    "        ntar_cov[e] = np.cov(ntar[:, s:e_], rowvar=False)\n",
    "        all_cov[e] = np.cov(input_signal[:, s:e_], rowvar=False)\n",
    "    return [tar_mean, ntar_mean, tar_cov, ntar_cov, all_cov]\n",
    "\n",
    "def plot_trunc_mean(tar_mean, ntar_mean, time_index, labels, save_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(tar_mean.shape[0]):\n",
    "        plt.plot(time_index, tar_mean[i], label=f'{labels[i]} (Target)')\n",
    "        plt.plot(time_index, ntar_mean[i], label=f'{labels[i]} (Non-Target)', linestyle='--')\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Truncated Mean ERP Signals\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"Mean.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_trunc_cov(cov_list, save_dir):\n",
    "    names = [\"Target\", \"Non-Target\", \"All\"]\n",
    "    for i, cov in enumerate(cov_list):\n",
    "        avg_cov = np.mean(cov, axis=0)\n",
    "        plt.imshow(avg_cov, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"Covariance - {names[i]}\")\n",
    "        plt.savefig(os.path.join(save_dir, f\"Covariance_{names[i]}.png\"))\n",
    "        plt.close()\n",
    "'''\n",
    "\n",
    "main_code = '''\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from HW8Fun import produce_trun_mean_cov, plot_trunc_mean, plot_trunc_cov\n",
    "\n",
    "bp_low = 0.5\n",
    "bp_upp = 6\n",
    "electrode_num = 16\n",
    "electrode_name_ls = ['F3', 'Fz', 'F4', 'T7', 'C3', 'Cz', 'C4', 'T8', 'CP3', 'CPz', 'CP4', 'P7', 'P3', 'Pz', 'P4', 'P8']\n",
    "parent_dir = '/Users/suhasdas/Documents/GitHub/BIOS-584'\n",
    "data_dir = os.path.join(parent_dir, 'data')\n",
    "subject = 'K114'\n",
    "session = '001_BCI_TRN'\n",
    "time_index = np.linspace(0, 800, 25)\n",
    "\n",
    "out_dir = os.path.join(parent_dir, 'K114_2')\n",
    "mat_path = os.path.join(data_dir, f\"{subject}_{session}_Truncated_Data_0.5_6.mat\")\n",
    "data = sio.loadmat(mat_path)\n",
    "signal = data['Signal']\n",
    "labels = np.squeeze(data['Type'], axis=1)\n",
    "\n",
    "results = produce_trun_mean_cov(signal, labels, electrode_num)\n",
    "tar_mean, ntar_mean, tar_cov, ntar_cov, all_cov = results\n",
    "\n",
    "plot_trunc_mean(tar_mean, ntar_mean, time_index, electrode_name_ls, out_dir)\n",
    "plot_trunc_cov([tar_cov, ntar_cov, all_cov], out_dir)\n",
    "'''\n",
    "\n",
    "#needed AI help with saving the files, and had to make a new folder K114_2, don't worry I didn't actually upload the dataset\n",
    "with open(os.path.join(target_dir, 'HW8Fun.py'), 'w') as f:\n",
    "    f.write(fun_code.strip())\n",
    "\n",
    "with open(os.path.join(target_dir, 'HW8_main.py'), 'w') as f:\n",
    "    f.write(main_code.strip())\n",
    "\n",
    "\n",
    "bp_low = 0.5\n",
    "bp_upp = 6\n",
    "electrode_num = 16\n",
    "electrode_name_ls = ['F3', 'Fz', 'F4', 'T7', 'C3', 'Cz', 'C4', 'T8', 'CP3', 'CPz', 'CP4', 'P7', 'P3', 'Pz', 'P4', 'P8']\n",
    "parent_dir = '/Users/suhasdas/Documents/GitHub/BIOS-584'\n",
    "data_dir = os.path.join(parent_dir, 'data')\n",
    "subject = 'K114'\n",
    "session = '001_BCI_TRN'\n",
    "time_index = np.linspace(0, 800, 25)\n",
    "\n",
    "mat_path = os.path.join(data_dir, f\"{subject}_{session}_Truncated_Data_0.5_6.mat\")\n",
    "data = sio.loadmat(mat_path)\n",
    "signal = data['Signal']\n",
    "labels = np.squeeze(data['Type'], axis=1)\n",
    "\n",
    "def produce_trun_mean_cov(input_signal, input_type, E_val):\n",
    "    length = input_signal.shape[1] // E_val\n",
    "    tar = input_signal[input_type == 1]\n",
    "    ntar = input_signal[input_type == -1]\n",
    "    tar_mean = np.zeros((E_val, length))\n",
    "    ntar_mean = np.zeros((E_val, length))\n",
    "    tar_cov = np.zeros((E_val, length, length))\n",
    "    ntar_cov = np.zeros((E_val, length, length))\n",
    "    all_cov = np.zeros((E_val, length, length))\n",
    "    for e in range(E_val):\n",
    "        s = e * length\n",
    "        e_ = (e + 1) * length\n",
    "        tar_mean[e] = np.mean(tar[:, s:e_], axis=0)\n",
    "        ntar_mean[e] = np.mean(ntar[:, s:e_], axis=0)\n",
    "        tar_cov[e] = np.cov(tar[:, s:e_], rowvar=False)\n",
    "        ntar_cov[e] = np.cov(ntar[:, s:e_], rowvar=False)\n",
    "        all_cov[e] = np.cov(input_signal[:, s:e_], rowvar=False)\n",
    "    return [tar_mean, ntar_mean, tar_cov, ntar_cov, all_cov]\n",
    "\n",
    "def plot_trunc_mean(tar_mean, ntar_mean, time_index, labels, save_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(tar_mean.shape[0]):\n",
    "        plt.plot(time_index, tar_mean[i], label=f'{labels[i]} (Target)')\n",
    "        plt.plot(time_index, ntar_mean[i], label=f'{labels[i]} (Non-Target)', linestyle='--')\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Truncated Mean ERP Signals\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"Mean.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_trunc_cov(cov_list, save_dir):\n",
    "    names = [\"Target\", \"Non-Target\", \"All\"]\n",
    "    for i, cov in enumerate(cov_list):\n",
    "        avg_cov = np.mean(cov, axis=0)\n",
    "        plt.imshow(avg_cov, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"Covariance - {names[i]}\")\n",
    "        plt.savefig(os.path.join(save_dir, f\"Covariance_{names[i]}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "results = produce_trun_mean_cov(signal, labels, electrode_num)\n",
    "tar_mean, ntar_mean, tar_cov, ntar_cov, all_cov = results\n",
    "\n",
    "plot_trunc_mean(tar_mean, ntar_mean, time_index, electrode_name_ls, target_dir)\n",
    "plot_trunc_cov([tar_cov, ntar_cov, all_cov], target_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf94138-c7f5-4e2b-ba01-7df9beb1d03d",
   "metadata": {},
   "source": [
    "https://github.com/sudashas/BIOS-584/tree/main/K114_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33469a50-fa2e-4b1d-bb2e-15e924437235",
   "metadata": {},
   "source": [
    "## Q2. A real-world data anlaysis using `Pandas` and `Scipy` (10')\n",
    "\n",
    "<font size='4'>\n",
    "\n",
    "- Back to the `PTSD dataset.xlsx`, let's import the dataset and name it `ptsd_df`. (no point since everyone has done it a couple of times before.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "031c92eb-a96d-410f-9724-443ba5431900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own code\n",
    "ptsd_dir = '{}/data/PTSD dataset.xlsx'.format(os.getcwd())\n",
    "ptsd_df = pd.read_excel(ptsd_dir, sheet_name=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f1981-d0d5-44e9-ac03-7f42a71a287a",
   "metadata": {},
   "source": [
    "### Q2.1. Univariate comparison (3')\n",
    "\n",
    "<font size='4'> \n",
    "    \n",
    "- Suppose that we would like to examine the utility/effect of an intervention program for patients with PTSD.\n",
    "- We measure PCL5 scores at completion (`pcl5week_score.completion`) and PCL5 score at 3-month follow-up (`pcl5month_score.3_month_follow_up`). Let's assume the first score is pre-intervention and the second score is post-intervention.\n",
    "- Report the summary statistics for each variable including mean, std, median, Q1, and Q3.\n",
    "- Note that each patient will receive such two PCL5 scores. Use a appropriate statistical test to perform the univariate comparison. Report the outputing statistic and p-value.\n",
    "- Before you run the statistic test, determine the data type and check the missingness of two columns. In particular, report the number of NA values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4deaacf5-b684-4c5f-8a35-728d899d8a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcl5week_score.completion            float64\n",
      "pcl5month_score.3_month_follow_up    float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "pcl5week_score.completion: 27 missing\n",
      "pcl5month_score.3_month_follow_up: 251 missing\n",
      "\n",
      "Summary Statistics:\n",
      "pcl5week_score.completion: {'mean': np.float64(27.209821428571427), 'std': np.float64(19.18486165078856), 'median': np.float64(22.0), 'Q1': np.float64(12.75), 'Q3': np.float64(41.0)}\n",
      "pcl5month_score.3_month_follow_up: {'mean': np.float64(32.026785714285715), 'std': np.float64(18.978609541146863), 'median': np.float64(31.0), 'Q1': np.float64(16.0), 'Q3': np.float64(46.0)}\n",
      "\n",
      "Paired t-test Results:\n",
      "t-statistic: -4.7092\n",
      "p-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Write your own code\n",
    "pre_col = 'pcl5week_score.completion'\n",
    "post_col = 'pcl5month_score.3_month_follow_up'\n",
    "pre_scores = ptsd_df[pre_col]\n",
    "post_scores = ptsd_df[post_col]\n",
    "\n",
    "#print to see the data types\n",
    "print(ptsd_df[[pre_col, post_col]].dtypes) #both float64, think that should be fine\n",
    "\n",
    "#to check missingness per the fifth bullet point \n",
    "print(\"\\nMissing Values:\")\n",
    "print(f\"{pre_col}: {pre_scores.isna().sum()} missing\")\n",
    "print(f\"{post_col}: {post_scores.isna().sum()} missing\")\n",
    "\n",
    "paired_df = ptsd_df[[pre_col, post_col]].dropna()\n",
    "pre = paired_df[pre_col]\n",
    "post = paired_df[post_col]\n",
    "\n",
    "#univariate statistics\n",
    "def summarize(series):\n",
    "    return {\n",
    "        'mean': np.mean(series),\n",
    "        'std': np.std(series, ddof=1),\n",
    "        'median': np.median(series),\n",
    "        'Q1': np.percentile(series, 25),\n",
    "        'Q3': np.percentile(series, 75)\n",
    "    }\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"{pre_col}: {summarize(pre)}\")\n",
    "print(f\"{post_col}: {summarize(post)}\")\n",
    "\n",
    "#t-test\n",
    "t_stat, p_val = stats.ttest_rel(pre, post)\n",
    "print(\"\\nPaired t-test Results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\") #t-statistic of -4.7092\n",
    "print(f\"p-value: {p_val:.4f}\") #statistically significant, very small p-value (didn't we do this for the quiz?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b8f8a-07d0-4f44-b024-f13cd8ed5e3f",
   "metadata": {},
   "source": [
    "### Q2.2. Multiple Linear Regression (7')\n",
    "\n",
    "<font size='4'>\n",
    "\n",
    "- Select columns specified in the following code chunk and create a subset dataset named `ptsd_sub_df`.\n",
    "- Fit a linear regression to examine the association between `caps_intake` (continuous outcome) and the remaining covariates (as predictors) using `ptsd_sub_df`.\n",
    "    - Note that all covariates ending with `_code` are categorical variables.\n",
    "- Use the instruction here to write the formula for linear regression in Python.\n",
    "    - https://www.statsmodels.org/stable/example_formulas.html\n",
    "- Report the output page including R2, adjusted R2, and parameter estimates, SE, 95% confidence intervals, and p-values.\n",
    "- Provide a brief interpretation for all significant predictors (p<0.05) excluding the intercept.\n",
    "- Relevant label information includes:\n",
    "    - `employment_code`: 1: Employed, 2: Unemployed, 3: Retired, 4: Disabled/Unable to work, 5: Student, 6: Other.\n",
    "    - `rank_code`: 1. Enlisted, 2: Officer, 3: Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2865a71e-6c03-4c83-9a31-38e01075ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following column names are used for linear regression.\n",
    "# Do not delete.\n",
    "relevant_col_names = ['caps_intake', 'age_iop', 'gender_code', 'sexualorient_code', 'race_code', 'ethnicity_code', \n",
    "                      'education_code', 'employment_code',\n",
    "                      'rank_code', 'branch_code', 'mdd_code', 'ctq_total_score', 'sexual_trauma', 'sud_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20eba4dd-df71-4a5d-b0b9-a12640f35ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            caps_intake   R-squared:                       0.119\n",
      "Model:                            OLS   Adj. R-squared:                  0.103\n",
      "Method:                 Least Squares   F-statistic:                     7.054\n",
      "Date:                Wed, 05 Nov 2025   Prob (F-statistic):           9.45e-09\n",
      "Time:                        14:09:23   Log-Likelihood:                -1491.0\n",
      "No. Observations:                 425   AIC:                             3000.\n",
      "Df Residuals:                     416   BIC:                             3036.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Intercept                    39.9779      3.099     12.899      0.000      33.886      46.070\n",
      "age_iop                      -0.0721      0.044     -1.630      0.104      -0.159       0.015\n",
      "gender_code                  -0.3564      0.843     -0.423      0.673      -2.014       1.301\n",
      "race_code                    -0.5767      0.280     -2.059      0.040      -1.127      -0.026\n",
      "ethnicity_code               -2.1443      1.062     -2.020      0.044      -4.231      -0.057\n",
      "employment_code               0.1461      0.259      0.565      0.573      -0.363       0.655\n",
      "rank_code                    -1.4436      0.828     -1.743      0.082      -3.072       0.185\n",
      "ctq_total_score               0.0495      0.019      2.593      0.010       0.012       0.087\n",
      "pcl5week_score_completion     0.1194      0.021      5.778      0.000       0.079       0.160\n",
      "==============================================================================\n",
      "Omnibus:                        1.289   Durbin-Watson:                   1.790\n",
      "Prob(Omnibus):                  0.525   Jarque-Bera (JB):                1.369\n",
      "Skew:                           0.097   Prob(JB):                        0.504\n",
      "Kurtosis:                       2.801   Cond. No.                         588.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                               Coef.  Std.Err.          t         P>|t|  \\\n",
      "Intercept                  39.977913  3.099338  12.898857  2.951316e-32   \n",
      "race_code                  -0.576667  0.280049  -2.059164  4.010049e-02   \n",
      "ethnicity_code             -2.144314  1.061695  -2.019708  4.405432e-02   \n",
      "ctq_total_score             0.049524  0.019098   2.593160  9.845136e-03   \n",
      "pcl5week_score_completion   0.119417  0.020666   5.778293  1.480999e-08   \n",
      "\n",
      "                              [0.025     0.975]  \n",
      "Intercept                  33.885598  46.070228  \n",
      "race_code                  -1.127156  -0.026179  \n",
      "ethnicity_code             -4.231271  -0.057358  \n",
      "ctq_total_score             0.011984   0.087065  \n",
      "pcl5week_score_completion   0.078793   0.160040  \n"
     ]
    }
   ],
   "source": [
    "# Write your own code\n",
    "#print(ptsd_df.columns.tolist())\n",
    "ptsd_df = ptsd_df.rename(columns={'pcl5week_score.completion': 'pcl5week_score_completion'})\n",
    "\n",
    "cols = [\n",
    "    'caps_intake',\n",
    "    'age_iop',\n",
    "    'gender_code',\n",
    "    'race_code',\n",
    "    'ethnicity_code',\n",
    "    'employment_code',\n",
    "    'rank_code',\n",
    "    'ctq_total_score',\n",
    "    'pcl5week_score_completion'\n",
    "]\n",
    "\n",
    "ptsd_sub_df = ptsd_df[cols].dropna() #to get rid of missing values\n",
    "\n",
    "model = smf.ols(\n",
    "    formula='caps_intake ~ age_iop + gender_code + race_code + ethnicity_code + employment_code + rank_code + ctq_total_score + pcl5week_score_completion',\n",
    "    data=ptsd_sub_df\n",
    ").fit()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "summary_df = model.summary2().tables[1]\n",
    "significant = summary_df[summary_df['P>|t|'] < 0.05]\n",
    "print(significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65f3afe5-d340-4ff0-8f10-4bc089ef6ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statisticaly significant predictors at the 95% confidence interval in this regression is race_code, ethnicity_code,ctq_total_score, and pcl5week_score_completion. This means that for every unit increase in caps_intake, there is a significant -2.059 in race code (if I'm being honest I haven't read the data dictionary so I don't know what that means), -2.020 decrease in ethnicity_code, 2.593 increase in ctq score, and 5.778 increase in pcl5week scores, adjusting for all other factors. \n"
     ]
    }
   ],
   "source": [
    "# Write your interpretations below:\n",
    "print(\"The statisticaly significant predictors at the 95% confidence interval in this regression is race_code, ethnicity_code,ctq_total_score, and pcl5week_score_completion. This means that for every unit increase in caps_intake, there is a significant -2.059 in race code (if I'm being honest I haven't read the data dictionary so I don't know what that means), -2.020 decrease in ethnicity_code, 2.593 increase in ctq score, and 5.778 increase in pcl5week scores, adjusting for all other factors. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103a5a3-ce40-4e3f-9bd4-b2a93910399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The statisticaly significant predictors at the 95% confidence interval in this regression is race_code, ethnicity_code,ctq_total_score, and pcl5week_score_completion. \n",
    "#This means that for every unit increase in caps_intake, there is a significant -2.059 in race code (if I'm being honest I haven't read the data dictionary so I don't know what that means), -2.020 decrease in ethnicity_code, 2.593 increase in ctq score, and 5.778 increase in pcl5week scores, adjusting for all other factors. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
